{
  cells : [
  {
    cell_type :  code ,
    execution_count : 4,
    metadata : {},
    outputs : [],
    source : [
     %matplotlib inline 
   ]
  },
  {
    cell_type :  markdown ,
    metadata : {},
    source : [
     \n ,
     PyTorch: nn\n ,
     ------------\n ,
     \n ,
     A fully-connected ReLU network with one hidden layer, trained to predict y from x\n ,
     by minimizing squared Euclidean distance.\n ,
     \n ,
     This implementation uses the nn package from PyTorch to build the network.\n ,
     PyTorch autograd makes it easy to define computational graphs and take gradients,\n ,
     but raw autograd can be a bit too low-level for defining complex neural networks;\n ,
     this is where the nn package can help. The nn package defines a set of Modules,\n ,
     which you can think of as a neural network layer that has produces output from\n ,
     input and may have some trainable weights.\n ,
     \n 
   ]
  },
  {
    cell_type :  code ,
    execution_count : 2,
    metadata : {
     scrolled : true
   },
    outputs : [
    {
      name :  stdout ,
      output_type :  stream ,
      text : [
       0 1.024683952331543\n ,
       1 0.9824423789978027\n ,
       2 0.9456233382225037\n ,
       3 0.9109653830528259\n ,
       4 0.8777695298194885\n ,
       5 0.845854640007019\n ,
       6 0.8151460289955139\n ,
       7 0.7855920195579529\n ,
       8 0.7571464776992798\n ,
       9 0.729766845703125\n ,
       10 0.703411340713501\n ,
       11 0.6780409812927246\n ,
       12 0.6536173820495605\n ,
       13 0.6301038861274719\n ,
       14 0.607465922832489\n ,
       15 0.5856699347496033\n ,
       16 0.5646830201148987\n ,
       17 0.5444746017456055\n ,
       18 0.5250149369239807\n ,
       19 0.5062749981880188\n ,
       20 0.488227516412735\n ,
       21 0.47084611654281616\n ,
       22 0.45410531759262085\n ,
       23 0.4379808306694031\n ,
       24 0.4224492311477661\n ,
       25 0.40748780965805054\n ,
       26 0.3930751085281372\n ,
       27 0.37919020652770996\n ,
       28 0.36581337451934814\n ,
       29 0.35292521119117737\n ,
       30 0.3405071794986725\n ,
       31 0.3285418748855591\n ,
       32 0.3170118033885956\n ,
       33 0.3059009909629822\n ,
       34 0.29519346356391907\n ,
       35 0.2848742604255676\n ,
       36 0.27492859959602356\n ,
       37 0.26534271240234375\n ,
       38 0.25610291957855225\n ,
       39 0.2471964806318283\n ,
       40 0.23861093819141388\n ,
       41 0.23033419251441956\n ,
       42 0.2223549485206604\n ,
       43 0.21466201543807983\n ,
       44 0.2072448432445526\n ,
       45 0.20009319484233856\n ,
       46 0.1931971162557602\n ,
       47 0.18654733896255493\n ,
       48 0.1801345944404602\n ,
       49 0.17395028471946716\n ,
       50 0.16798590123653412\n ,
       51 0.1622333526611328\n ,
       52 0.15668490529060364\n ,
       53 0.1513330042362213\n ,
       54 0.1461704969406128\n ,
       55 0.14119037985801697\n ,
       56 0.13638609647750854\n ,
       57 0.13175110518932343\n ,
       58 0.12727922201156616\n ,
       59 0.12296461313962936\n ,
       60 0.1188015267252922\n ,
       61 0.11478441208600998\n ,
       62 0.11090796440839767\n ,
       63 0.10716710239648819\n ,
       64 0.10355690121650696\n ,
       65 0.10007266700267792\n ,
       66 0.09670979529619217\n ,
       67 0.09346398711204529\n ,
       68 0.09033097326755524\n ,
       69 0.08730670064687729\n ,
       70 0.08438722789287567\n ,
       71 0.0815688744187355\n ,
       72 0.07884793728590012\n ,
       73 0.07622098177671432\n ,
       74 0.07368461042642593\n ,
       75 0.07123562693595886\n ,
       76 0.06887084990739822\n ,
       77 0.06658737361431122\n ,
       78 0.06438220292329788\n ,
       79 0.06225261092185974\n ,
       80 0.06019590422511101\n ,
       81 0.05820951238274574\n ,
       82 0.056290991604328156\n ,
       83 0.0544377937912941\n ,
       84 0.05264779552817345\n ,
       85 0.05091862753033638\n ,
       86 0.049248214811086655\n ,
       87 0.04763449355959892\n ,
       88 0.04607543721795082\n ,
       89 0.04456913098692894\n ,
       90 0.04311375319957733\n ,
       91 0.04170753434300423\n ,
       92 0.040348682552576065\n ,
       93 0.0390356108546257\n ,
       94 0.0377667210996151\n ,
       95 0.03654045611619949\n ,
       96 0.035355329513549805\n ,
       97 0.03420994430780411\n ,
       98 0.03310287743806839\n ,
       99 0.0320328064262867\n ,
       100 0.030998490750789642\n ,
       101 0.0299986582249403\n ,
       102 0.029032131657004356\n ,
       103 0.028097759932279587\n ,
       104 0.027194444090127945\n ,
       105 0.026321107521653175\n ,
       106 0.025476742535829544\n ,
       107 0.02466031163930893\n ,
       108 0.023870905861258507\n ,
       109 0.023107562214136124\n ,
       110 0.022369414567947388\n ,
       111 0.021655596792697906\n ,
       112 0.02096525765955448\n ,
       113 0.020297637209296227\n ,
       114 0.019651949405670166\n ,
       115 0.019027413800358772\n ,
       116 0.01842338591814041\n ,
       117 0.017839111387729645\n ,
       118 0.017273899167776108\n ,
       119 0.01672719046473503\n ,
       120 0.016198307275772095\n ,
       121 0.015686657279729843\n ,
       122 0.015191656537353992\n ,
       123 0.0147127415984869\n ,
       124 0.01424938440322876\n ,
       125 0.01380105596035719\n ,
       126 0.013367259874939919\n ,
       127 0.01294749602675438\n ,
       128 0.01254130806773901\n ,
       129 0.012148250825703144\n ,
       130 0.011767849326133728\n ,
       131 0.011399711482226849\n ,
       132 0.011043429374694824\n ,
       133 0.010698609054088593\n ,
       134 0.010364850983023643\n ,
       135 0.010041818022727966\n ,
       136 0.009729131124913692\n ,
       137 0.009426466189324856\n ,
       138 0.009133475832641125\n ,
       139 0.008849848993122578\n ,
       140 0.00857526808977127\n ,
       141 0.008309459313750267\n ,
       142 0.008052103221416473\n ,
       143 0.007802929263561964\n ,
       144 0.007561695296317339\n ,
       145 0.0073281144723296165\n ,
       146 0.007101953495293856\n ,
       147 0.006882956717163324\n ,
       148 0.0066708908416330814\n ,
       149 0.006465529557317495\n ,
       150 0.006266660522669554\n ,
       151 0.006074069533497095\n ,
       152 0.005887557286769152\n ,
       153 0.005706913769245148\n ,
       154 0.0055319624952971935\n ,
       155 0.005362514406442642\n ,
       156 0.005198388826102018\n ,
       157 0.005039402283728123\n ,
       158 0.00488541042432189\n ,
       159 0.004736253526061773\n ,
       160 0.004591753240674734\n ,
       161 0.004451777786016464\n ,
       162 0.004316161386668682\n ,
       163 0.004184795543551445\n ,
       164 0.004057521000504494\n ,
       165 0.003934203181415796\n ,
       166 0.003814737079665065\n ,
       167 0.003698976244777441\n ,
       168 0.003586818929761648\n ,
       169 0.0034781380090862513\n ,
       170 0.0033728373236954212\n ,
       171 0.0032707923091948032\n ,
       172 0.003171910997480154\n ,
       173 0.0030760744120925665\n ,
       174 0.00298321433365345\n ,
       175 0.0028932280838489532\n ,
       176 0.0028060090262442827\n ,
       177 0.0027214759029448032\n ,
       178 0.002639554440975189\n ,
       179 0.0025601512752473354\n ,
       180 0.002483196323737502\n ,
       181 0.0024085959885269403\n ,
       182 0.002336282981559634\n ,
       183 0.0022662021219730377\n ,
       184 0.0021982677280902863\n ,
       185 0.0021324148401618004\n ,
       186 0.0020685666240751743\n ,
       187 0.00200667604804039\n ,
       188 0.0019466825760900974\n ,
       189 0.0018885141471400857\n ,
       190 0.0018321212846785784\n ,
       191 0.0017774500884115696\n ,
       192 0.0017244472401216626\n ,
       193 0.0016730513889342546\n ,
       194 0.0016232198104262352\n ,
       195 0.001574902911670506\n ,
       196 0.0015280551742762327\n ,
       197 0.0014826355036348104\n ,
       198 0.0014385846443474293\n ,
       199 0.001395872444845736\n ,
       200 0.0013544491957873106\n ,
       201 0.0013142864918336272\n ,
       202 0.0012753339251503348\n ,
       203 0.0012375551741570234\n ,
       204 0.0012009227648377419\n ,
       205 0.0011653956025838852\n ,
       206 0.0011309337569400668\n ,
       207 0.0010975180193781853\n ,
       208 0.0010651035699993372\n ,
       209 0.001033661188557744\n ,
       210 0.001003172597847879\n ,
       211 0.0009735944913700223\n ,
       212 0.0009449115605093539\n ,
       213 0.0009170841076411307\n ,
       214 0.0008900901884771883\n ,
       215 0.0008639116422273219\n ,
       216 0.0008385091787204146\n ,
       217 0.0008138727862387896\n ,
       218 0.0007899706833995879\n ,
       219 0.0007667827885597944\n ,
       220 0.0007442904752679169\n ,
       221 0.000722466385923326\n ,
       222 0.000701294862665236\n ,
       223 0.0006807551835663617\n ,
       224 0.0006608283147215843\n ,
       225 0.0006414941162802279\n ,
       226 0.0006227323319762945\n ,
       227 0.0006045353366062045\n ,
       228 0.0005868739099241793\n ,
       229 0.0005697397282347083\n ,
       230 0.0005531145143322647\n ,
       231 0.0005369819118641317\n ,
       232 0.0005213276599533856\n ,
       233 0.0005061397096142173\n ,
       234 0.0004913986194878817\n ,
       235 0.00047709583304822445\n ,
       236 0.0004632161871995777\n ,
       237 0.0004497453919611871\n ,
       238 0.00043667599675245583\n ,
       239 0.00042399176163598895\n ,
       240 0.0004116799682378769\n ,
       241 0.0003997342719230801\n ,
       242 0.0003881372103933245\n ,
       243 0.0003768833412323147\n ,
       244 0.00036596375866793096\n ,
       245 0.000355365191353485\n ,
       246 0.00034507675445638597\n ,
       247 0.00033509288914501667\n ,
       248 0.00032540049869567156\n ,
       249 0.00031599754584021866\n ,
       250 0.0003068653750233352\n ,
       251 0.0002980037243105471\n ,
       252 0.0002894029312301427\n ,
       253 0.00028105315868742764\n ,
       254 0.0002729478292167187\n ,
       255 0.00026508080190978944\n ,
       256 0.00025744520826265216\n ,
       257 0.00025003249174915254\n ,
       258 0.0002428356820018962\n ,
       259 0.00023584977316204458\n ,
       260 0.00022907016682438552\n ,
       261 0.0002224869531346485\n ,
       262 0.00021609588293358684\n ,
       263 0.0002098932018270716\n ,
       264 0.0002038694656221196\n ,
       265 0.0001980218366952613\n ,
       266 0.00019234625506214797\n ,
       267 0.00018683502275962383\n ,
       268 0.00018148263916373253\n ,
       269 0.00017628872592467815\n ,
       270 0.00017124605074059218\n ,
       271 0.00016634933126624674\n ,
       272 0.0001615959918126464\n ,
       273 0.0001569773448863998\n ,
       274 0.00015249681018758565\n ,
       275 0.00014814466703683138\n ,
       276 0.00014391863078344613\n ,
       277 0.00013981647498439997\n ,
       278 0.00013583226245827973\n ,
       279 0.00013196462532505393\n ,
       280 0.0001282073644688353\n ,
       281 0.00012456096010282636\n ,
       282 0.00012101999163860455\n ,
       283 0.00011758023174479604\n ,
       284 0.00011424112744862214\n ,
       285 0.00011099790572188795\n ,
       286 0.00010784870391944423\n ,
       287 0.00010479033517185599\n ,
       288 0.00010182090773014352\n ,
       289 9.893588139675558e-05\n ,
       290 9.613499423721805e-05\n ,
       291 9.34155032155104e-05\n ,
       292 9.077358117792755e-05\n ,
       293 8.820719813229516e-05\n ,
       294 8.571650687372312e-05\n ,
       295 8.329594129463658e-05\n ,
       296 8.094595250440761e-05\n ,
       297 7.866309897508472e-05\n ,
       298 7.644564175279811e-05\n ,
       299 7.429246761603281e-05\n ,
       300 7.220082625281066e-05\n ,
       301 7.016854942776263e-05\n ,
       302 6.819518603151664e-05\n ,
       303 6.627893162658438e-05\n ,
       304 6.441798177547753e-05\n ,
       305 6.260845111683011e-05\n ,
       306 6.085184941184707e-05\n ,
       307 5.914497160119936e-05\n ,
       308 5.7487657613819465e-05\n ,
       309 5.5876509577501565e-05\n ,
       310 5.4312844440573826e-05\n ,
       311 5.279218021314591e-05\n ,
       312 5.1316550525370985e-05\n ,
       313 4.988168439012952e-05\n ,
       314 4.848801836487837e-05\n ,
       315 4.7134763008216396e-05\n ,
       316 4.581850589602254e-05\n ,
       317 4.454109148355201e-05\n ,
       318 4.329924922785722e-05\n ,
       319 4.209357575746253e-05\n ,
       320 4.0920276660472155e-05\n ,
       321 3.9782269595889375e-05\n ,
       322 3.8675811083521694e-05\n ,
       323 3.759961691685021e-05\n ,
       324 3.655462933238596e-05\n ,
       325 3.5539815144147724e-05\n ,
       326 3.455289333942346e-05\n ,
       327 3.3593973057577386e-05\n ,
       328 3.266253406764008e-05\n ,
       329 3.1757175747770816e-05\n ,
       330 3.0877657991368324e-05\n 
     ]
    },
    {
      name :  stdout ,
      output_type :  stream ,
      text : [
       331 3.002266021212563e-05\n ,
       332 2.9191121939220466e-05\n ,
       333 2.838377986336127e-05\n ,
       334 2.759855124168098e-05\n ,
       335 2.6835541575565003e-05\n ,
       336 2.6094061468029395e-05\n ,
       337 2.537401087465696e-05\n ,
       338 2.4673150619491935e-05\n ,
       339 2.3992488422663882e-05\n ,
       340 2.333072006877046e-05\n ,
       341 2.2687632736051455e-05\n ,
       342 2.2062728021410294e-05\n ,
       343 2.145434518752154e-05\n ,
       344 2.086417043756228e-05\n ,
       345 2.0290444808779284e-05\n ,
       346 1.9731533029698767e-05\n ,
       347 1.918900125019718e-05\n ,
       348 1.8661186913959682e-05\n ,
       349 1.8148828530684114e-05\n ,
       350 1.765040542522911e-05\n ,
       351 1.7166001271107234e-05\n ,
       352 1.6694744772394188e-05\n ,
       353 1.6236899682553485e-05\n ,
       354 1.579156014486216e-05\n ,
       355 1.5358773453044705e-05\n ,
       356 1.4937937521608546e-05\n ,
       357 1.4528935025737155e-05\n ,
       358 1.4131079296930693e-05\n ,
       359 1.3744795069214888e-05\n ,
       360 1.3368618965614587e-05\n ,
       361 1.300268922932446e-05\n ,
       362 1.2647533367271535e-05\n ,
       363 1.230187262990512e-05\n ,
       364 1.1965452358708717e-05\n ,
       365 1.1638921932899393e-05\n ,
       366 1.1321077181492e-05\n ,
       367 1.101231828215532e-05\n ,
       368 1.071200858859811e-05\n ,
       369 1.0419751561130397e-05\n ,
       370 1.0135621778317727e-05\n ,
       371 9.859534657152835e-06\n ,
       372 9.591024536348414e-06\n ,
       373 9.32962302613305e-06\n ,
       374 9.075905836652964e-06\n ,
       375 8.828722457110416e-06\n ,
       376 8.58825023897225e-06\n ,
       377 8.354910278285388e-06\n ,
       378 8.127889486786444e-06\n ,
       379 7.907049621280748e-06\n ,
       380 7.692263352510054e-06\n ,
       381 7.483200988644967e-06\n ,
       382 7.280250883923145e-06\n ,
       383 7.082318916218355e-06\n ,
       384 6.89001672071754e-06\n ,
       385 6.703373855998507e-06\n ,
       386 6.521441719087306e-06\n ,
       387 6.3448428591073025e-06\n ,
       388 6.172633675305406e-06\n ,
       389 6.005390332575189e-06\n ,
       390 5.843005965289194e-06\n ,
       391 5.684772986569442e-06\n ,
       392 5.530772796191741e-06\n ,
       393 5.380923539632931e-06\n ,
       394 5.23523385709268e-06\n ,
       395 5.0938497224706225e-06\n ,
       396 4.955913482262986e-06\n ,
       397 4.822359187528491e-06\n ,
       398 4.69170527139795e-06\n ,
       399 4.565006292978069e-06\n ,
       400 4.441903001861647e-06\n ,
       401 4.321717369748512e-06\n ,
       402 4.2051055970659945e-06\n ,
       403 4.09168114856584e-06\n ,
       404 3.981246663897764e-06\n ,
       405 3.873746663884958e-06\n ,
       406 3.7694148886657786e-06\n ,
       407 3.667802957352251e-06\n ,
       408 3.5688660773303127e-06\n ,
       409 3.4727527236100286e-06\n ,
       410 3.3791930036386475e-06\n ,
       411 3.2881844163057394e-06\n ,
       412 3.199514367224765e-06\n ,
       413 3.1135011795413448e-06\n ,
       414 3.029394974873867e-06\n ,
       415 2.9481875571946148e-06\n ,
       416 2.8689623832178768e-06\n ,
       417 2.791721499306732e-06\n ,
       418 2.71664453066478e-06\n ,
       419 2.6435693598614307e-06\n ,
       420 2.57267629422131e-06\n ,
       421 2.503485802662908e-06\n ,
       422 2.436398517602356e-06\n ,
       423 2.3707727905275533e-06\n ,
       424 2.307323939021444e-06\n ,
       425 2.2453150450019166e-06\n ,
       426 2.1850530629308196e-06\n ,
       427 2.126309709638008e-06\n ,
       428 2.0695097191492096e-06\n ,
       429 2.013874563999707e-06\n ,
       430 1.959868995982106e-06\n ,
       431 1.9073950170422904e-06\n ,
       432 1.8561447632237105e-06\n ,
       433 1.8064965843223035e-06\n ,
       434 1.758192183842766e-06\n ,
       435 1.7111894976551412e-06\n ,
       436 1.6652700196573278e-06\n ,
       437 1.6208279021157068e-06\n ,
       438 1.577224111315445e-06\n ,
       439 1.5351168940469506e-06\n ,
       440 1.4940301298338454e-06\n ,
       441 1.4541273003487731e-06\n ,
       442 1.4151597724776366e-06\n ,
       443 1.3773809541817172e-06\n ,
       444 1.3405970094026998e-06\n ,
       445 1.304705051552446e-06\n ,
       446 1.2699189255727106e-06\n ,
       447 1.235910758623504e-06\n ,
       448 1.2028822311549447e-06\n ,
       449 1.1708469855875592e-06\n ,
       450 1.139578898801119e-06\n ,
       451 1.1091170790678007e-06\n ,
       452 1.0795753269121633e-06\n ,
       453 1.0508177865631296e-06\n ,
       454 1.0227088296232978e-06\n ,
       455 9.955159612218267e-07\n ,
       456 9.690917295301915e-07\n ,
       457 9.432537240172678e-07\n ,
       458 9.18098180591187e-07\n ,
       459 8.935923005992663e-07\n ,
       460 8.698012834429392e-07\n ,
       461 8.465485734632239e-07\n ,
       462 8.240251645474927e-07\n ,
       463 8.020660970942117e-07\n ,
       464 7.806285680089786e-07\n ,
       465 7.599932700941281e-07\n ,
       466 7.397377999041055e-07\n ,
       467 7.20101809292828e-07\n ,
       468 7.00879695614276e-07\n ,
       469 6.822691602792474e-07\n ,
       470 6.641077447966381e-07\n ,
       471 6.464666171268618e-07\n ,
       472 6.292268608376617e-07\n ,
       473 6.125840172899188e-07\n ,
       474 5.962444333817984e-07\n ,
       475 5.804482725579874e-07\n ,
       476 5.651072001455759e-07\n ,
       477 5.5002931276249e-07\n ,
       478 5.354744985197613e-07\n ,
       479 5.212058340475778e-07\n ,
       480 5.073805482425087e-07\n ,
       481 4.93957315939042e-07\n ,
       482 4.808201765627018e-07\n ,
       483 4.680549920976773e-07\n ,
       484 4.5567563233817054e-07\n ,
       485 4.435954110704188e-07\n ,
       486 4.317854234159313e-07\n ,
       487 4.203654100365384e-07\n ,
       488 4.092482868145453e-07\n ,
       489 3.9839818555265083e-07\n ,
       490 3.8784159528404416e-07\n ,
       491 3.775360255531268e-07\n ,
       492 3.6750665799445414e-07\n ,
       493 3.578020653094427e-07\n ,
       494 3.483180250896112e-07\n ,
       495 3.391335212654667e-07\n ,
       496 3.301774711417238e-07\n ,
       497 3.21373505585143e-07\n ,
       498 3.1288413993024733e-07\n ,
       499 3.0460844868684944e-07\n 
     ]
    }
   ],
    source : [
     import torch\n ,
     import numpy as np\n ,
     \n ,
     # N is batch size; D_in is input dimension;\n ,
     # H is hidden dimension; D_out is output dimension.\n ,
     N, D_in, H, D_out = 64, 1000, 50000, 1\n ,
     \n ,
     # Create random Tensors to hold inputs and outputs\n ,
     torch.manual_seed(1000)\n ,
     x = torch.randn(N, D_in)\n ,
     #y = torch.randn(N, D_out)\n ,
     y = torch.randn(N, D_out)\n ,
     # Use the nn package to define our model as a sequence of layers. nn.Sequential\n ,
     # is a Module which contains other Modules, and applies them in sequence to\n ,
     # produce its output. Each Linear Module computes output from input using a\n ,
     # linear function, and holds internal Tensors for its weight and bias.\n ,
     model = torch.nn.Sequential(\n ,
         torch.nn.Linear(D_in, H),\n ,
         torch.nn.ReLU(),\n ,
         torch.nn.Linear(H, D_out),\n ,
     )\n ,
     \n ,
     # The nn package also contains definitions of popular loss functions; in this\n ,
     # case we will use Mean Squared Error (MSE) as our loss function.\n ,
     #loss_fn = torch.nn.MSELoss(reduction='sum')\n ,
     loss_fn = torch.nn.MSELoss()\n ,
     \n ,
     learning_rate = 1e-4\n ,
     for t in range(500):\n ,
         # Forward pass: compute predicted y by passing x to the model. Module objects\n ,
         # override the __call__ operator so you can call them like functions. When\n ,
         # doing so you pass a Tensor of input data to the Module and it produces\n ,
         # a Tensor of output data.\n ,
         y_pred = model(x)\n ,
     \n ,
         # Compute and print loss. We pass Tensors containing the predicted and true\n ,
         # values of y, and the loss function returns a Tensor containing the\n ,
         # loss.\n ,
         loss = loss_fn(y_pred, y)\n ,
         #print(loss)\n ,
         #vals = tf.constant(loss)\n ,
         #vals = tf.gather(loss,0)\n ,
         #print(vals)\n ,
         #print(np.mean(vals))\n ,
         print(t,loss.item())\n ,
     \n ,
         # Zero the gradients before running the backward pass.\n ,
         model.zero_grad()\n ,
     \n ,
         # Backward pass: compute gradient of the loss with respect to all the learnable\n ,
         # parameters of the model. Internally, the parameters of each Module are stored\n ,
         # in Tensors with requires_grad=True, so this call will compute gradients for\n ,
         # all learnable parameters in the model.\n ,
         loss.backward()\n ,
     \n ,
         # Update the weights using gradient descent. Each parameter is a Tensor, so\n ,
         # we can access its gradients like we did before.\n ,
         with torch.no_grad():\n ,
             for param in model.parameters():\n ,
                 param -= learning_rate * param.grad 
   ]
  },
  {
    cell_type :  code ,
    execution_count : null,
    metadata : {},
    outputs : [],
    source : []
  }
 ],
  metadata : {
   kernelspec : {
    display_name :  Python 3 ,
    language :  python ,
    name :  python3 
  },
   language_info : {
    codemirror_mode : {
     name :  ipython ,
     version : 3
   },
    file_extension :  .py ,
    mimetype :  text/x-python ,
    name :  python ,
    nbconvert_exporter :  python ,
    pygments_lexer :  ipython3 ,
    version :  3.6.5 
  }
 },
  nbformat : 4,
  nbformat_minor : 1
}
